{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iD5tJAA1Fzb",
        "outputId": "1246655c-0523-4482-d809-159a150f03d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Printando os dados das Plantas Iris para Treino\n",
            "['1.9', '45', '77', '21.32963989', 'Normal', '']\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(linha)\n\u001b[0;32m     17\u001b[0m     coluna \u001b[39m=\u001b[39m linha[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     entrada\u001b[39m.\u001b[39mappend([\u001b[39mfloat\u001b[39m(coluna[\u001b[39m0\u001b[39m]),\u001b[39mfloat\u001b[39m(coluna[\u001b[39m3\u001b[39;49m])])\n\u001b[0;32m     19\u001b[0m     saidaDesejada\u001b[39m.\u001b[39mappend(coluna[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQuantidade de dados: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(entrada))\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Carga dos dados de Treino\n",
        "entrada =[]\n",
        "saidaDesejada = []\n",
        "\n",
        "print(\"Printando os dados das Plantas Iris para Treino\")\n",
        "with open('DadosDeTreino.csv') as csvfile:\n",
        "  dados = csv.reader(csvfile)\n",
        "  for linha in dados:\n",
        "    print(linha)\n",
        "    coluna = linha[0].split(',')\n",
        "    entrada.append([float(coluna[0]),float(coluna[3])])\n",
        "    saidaDesejada.append(coluna[4].replace('\"',''))\n",
        "\n",
        "print(\"Quantidade de dados: \", len(entrada))\n",
        "for ind in range(0,len(entrada)):\n",
        "  print(\"Features de entrada: \", entrada[ind],\"-\", \"Classe: \" + saidaDesejada[ind])\n",
        "\n",
        "csvfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zqY7pprTP80Q",
        "outputId": "9dbb6fef-af7d-4085-cd79-1425e50ee028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinando...\n",
            "Iteration 1, loss = 2.08856259\n",
            "Iteration 2, loss = 1.90734571\n",
            "Iteration 3, loss = 1.74173211\n",
            "Iteration 4, loss = 1.58845136\n",
            "Iteration 5, loss = 1.44889120\n",
            "Iteration 6, loss = 1.32146674\n",
            "Iteration 7, loss = 1.20599215\n",
            "Iteration 8, loss = 1.10382071\n",
            "Iteration 9, loss = 1.01708877\n",
            "Iteration 10, loss = 0.94660278\n",
            "Iteration 11, loss = 0.89271577\n",
            "Iteration 12, loss = 0.85541368\n",
            "Iteration 13, loss = 0.83367796\n",
            "Iteration 14, loss = 0.82462837\n",
            "Iteration 15, loss = 0.82310887\n",
            "Iteration 16, loss = 0.82357424\n",
            "Iteration 17, loss = 0.82120410\n",
            "Iteration 18, loss = 0.81291251\n",
            "Iteration 19, loss = 0.79809569\n",
            "Iteration 20, loss = 0.77690408\n",
            "Iteration 21, loss = 0.75076306\n",
            "Iteration 22, loss = 0.72155020\n",
            "Iteration 23, loss = 0.69108190\n",
            "Iteration 24, loss = 0.66134130\n",
            "Iteration 25, loss = 0.63449489\n",
            "Iteration 26, loss = 0.61257059\n",
            "Iteration 27, loss = 0.59509972\n",
            "Iteration 28, loss = 0.58097775\n",
            "Iteration 29, loss = 0.56939197\n",
            "Iteration 30, loss = 0.55930257\n",
            "Iteration 31, loss = 0.54948971\n",
            "Iteration 32, loss = 0.53899041\n",
            "Iteration 33, loss = 0.52745639\n",
            "Iteration 34, loss = 0.51511556\n",
            "Iteration 35, loss = 0.50268588\n",
            "Iteration 36, loss = 0.49100202\n",
            "Iteration 37, loss = 0.48073952\n",
            "Iteration 38, loss = 0.47213465\n",
            "Iteration 39, loss = 0.46478234\n",
            "Iteration 40, loss = 0.45795575\n",
            "Iteration 41, loss = 0.45089116\n",
            "Iteration 42, loss = 0.44322976\n",
            "Iteration 43, loss = 0.43513998\n",
            "Iteration 44, loss = 0.42711738\n",
            "Iteration 45, loss = 0.41964311\n",
            "Iteration 46, loss = 0.41299089\n",
            "Iteration 47, loss = 0.40700672\n",
            "Iteration 48, loss = 0.40132700\n",
            "Iteration 49, loss = 0.39568992\n",
            "Iteration 50, loss = 0.38965917\n",
            "Iteration 51, loss = 0.38361771\n",
            "Iteration 52, loss = 0.37764652\n",
            "Iteration 53, loss = 0.37190479\n",
            "Iteration 54, loss = 0.36643790\n",
            "Iteration 55, loss = 0.36117058\n",
            "Iteration 56, loss = 0.35576409\n",
            "Iteration 57, loss = 0.34991285\n",
            "Iteration 58, loss = 0.34388788\n",
            "Iteration 59, loss = 0.33796831\n",
            "Iteration 60, loss = 0.33221792\n",
            "Iteration 61, loss = 0.32641093\n",
            "Iteration 62, loss = 0.32023705\n",
            "Iteration 63, loss = 0.31374142\n",
            "Iteration 64, loss = 0.30725123\n",
            "Iteration 65, loss = 0.30078744\n",
            "Iteration 66, loss = 0.29392092\n",
            "Iteration 67, loss = 0.28669233\n",
            "Iteration 68, loss = 0.27955645\n",
            "Iteration 69, loss = 0.27248860\n",
            "Iteration 70, loss = 0.26518327\n",
            "Iteration 71, loss = 0.25764245\n",
            "Iteration 72, loss = 0.25034298\n",
            "Iteration 73, loss = 0.24321698\n",
            "Iteration 74, loss = 0.23585776\n",
            "Iteration 75, loss = 0.22868784\n",
            "Iteration 76, loss = 0.22180150\n",
            "Iteration 77, loss = 0.21479237\n",
            "Iteration 78, loss = 0.20806247\n",
            "Iteration 79, loss = 0.20158549\n",
            "Iteration 80, loss = 0.19510500\n",
            "Iteration 81, loss = 0.18898933\n",
            "Iteration 82, loss = 0.18305112\n",
            "Iteration 83, loss = 0.17725680\n",
            "Iteration 84, loss = 0.17183529\n",
            "Iteration 85, loss = 0.16651484\n",
            "Iteration 86, loss = 0.16149651\n",
            "Iteration 87, loss = 0.15672687\n",
            "Iteration 88, loss = 0.15212689\n",
            "Iteration 89, loss = 0.14784517\n",
            "Iteration 90, loss = 0.14368863\n",
            "Iteration 91, loss = 0.13982813\n",
            "Iteration 92, loss = 0.13611996\n",
            "Iteration 93, loss = 0.13265326\n",
            "Iteration 94, loss = 0.12935591\n",
            "Iteration 95, loss = 0.12624682\n",
            "Iteration 96, loss = 0.12333502\n",
            "Iteration 97, loss = 0.12054619\n",
            "Iteration 98, loss = 0.11794670\n",
            "Iteration 99, loss = 0.11547624\n",
            "Iteration 100, loss = 0.11316617\n",
            "Iteration 101, loss = 0.11097033\n",
            "Iteration 102, loss = 0.10891782\n",
            "Iteration 103, loss = 0.10696631\n",
            "Iteration 104, loss = 0.10513849\n",
            "Iteration 105, loss = 0.10340016\n",
            "Iteration 106, loss = 0.10176996\n",
            "Iteration 107, loss = 0.10021868\n",
            "Iteration 108, loss = 0.09876085\n",
            "Iteration 109, loss = 0.09737361\n",
            "Iteration 110, loss = 0.09606615\n",
            "Iteration 111, loss = 0.09482174\n",
            "Iteration 112, loss = 0.09364536\n",
            "Iteration 113, loss = 0.09252673\n",
            "Iteration 114, loss = 0.09146520\n",
            "Iteration 115, loss = 0.09045672\n",
            "Iteration 116, loss = 0.08949569\n",
            "Iteration 117, loss = 0.08858319\n",
            "Iteration 118, loss = 0.08771015\n",
            "Iteration 119, loss = 0.08688121\n",
            "Iteration 120, loss = 0.08608608\n",
            "Iteration 121, loss = 0.08532994\n",
            "Iteration 122, loss = 0.08460386\n",
            "Iteration 123, loss = 0.08391140\n",
            "Iteration 124, loss = 0.08324743\n",
            "Iteration 125, loss = 0.08261184\n",
            "Iteration 126, loss = 0.08200297\n",
            "Iteration 127, loss = 0.08141746\n",
            "Iteration 128, loss = 0.08085673\n",
            "Iteration 129, loss = 0.08031682\n",
            "Iteration 130, loss = 0.07979918\n",
            "Iteration 131, loss = 0.07930233\n",
            "Iteration 132, loss = 0.07882320\n",
            "Iteration 133, loss = 0.07836124\n",
            "Iteration 134, loss = 0.07791497\n",
            "Iteration 135, loss = 0.07748427\n",
            "Iteration 136, loss = 0.07706815\n",
            "Iteration 137, loss = 0.07666600\n",
            "Iteration 138, loss = 0.07627733\n",
            "Iteration 139, loss = 0.07590175\n",
            "Iteration 140, loss = 0.07553803\n",
            "Iteration 141, loss = 0.07518549\n",
            "Iteration 142, loss = 0.07484360\n",
            "Iteration 143, loss = 0.07451187\n",
            "Iteration 144, loss = 0.07418981\n",
            "Iteration 145, loss = 0.07387699\n",
            "Iteration 146, loss = 0.07357298\n",
            "Iteration 147, loss = 0.07327739\n",
            "Iteration 148, loss = 0.07298989\n",
            "Iteration 149, loss = 0.07271024\n",
            "Iteration 150, loss = 0.07243813\n",
            "Iteration 151, loss = 0.07217301\n",
            "Iteration 152, loss = 0.07191469\n",
            "Iteration 153, loss = 0.07166287\n",
            "Iteration 154, loss = 0.07141726\n",
            "Iteration 155, loss = 0.07117756\n",
            "Iteration 156, loss = 0.07094348\n",
            "Iteration 157, loss = 0.07071495\n",
            "Iteration 158, loss = 0.07049171\n",
            "Iteration 159, loss = 0.07027353\n",
            "Iteration 160, loss = 0.07006020\n",
            "Iteration 161, loss = 0.06985157\n",
            "Iteration 162, loss = 0.06964744\n",
            "Iteration 163, loss = 0.06944765\n",
            "Iteration 164, loss = 0.06925204\n",
            "Iteration 165, loss = 0.06906045\n",
            "Iteration 166, loss = 0.06887294\n",
            "Iteration 167, loss = 0.06868920\n",
            "Iteration 168, loss = 0.06850907\n",
            "Iteration 169, loss = 0.06833243\n",
            "Iteration 170, loss = 0.06815915\n",
            "Iteration 171, loss = 0.06798912\n",
            "Iteration 172, loss = 0.06782223\n",
            "Iteration 173, loss = 0.06765838\n",
            "Iteration 174, loss = 0.06749746\n",
            "Iteration 175, loss = 0.06733939\n",
            "Iteration 176, loss = 0.06718406\n",
            "Iteration 177, loss = 0.06703145\n",
            "Iteration 178, loss = 0.06688142\n",
            "Iteration 179, loss = 0.06673385\n",
            "Iteration 180, loss = 0.06658870\n",
            "Iteration 181, loss = 0.06644590\n",
            "Iteration 182, loss = 0.06630537\n",
            "Iteration 183, loss = 0.06616709\n",
            "Iteration 184, loss = 0.06603096\n",
            "Iteration 185, loss = 0.06589691\n",
            "Iteration 186, loss = 0.06576490\n",
            "Iteration 187, loss = 0.06563486\n",
            "Iteration 188, loss = 0.06550673\n",
            "Iteration 189, loss = 0.06538047\n",
            "Iteration 190, loss = 0.06525607\n",
            "Iteration 191, loss = 0.06513345\n",
            "Iteration 192, loss = 0.06501255\n",
            "Iteration 193, loss = 0.06489333\n",
            "Iteration 194, loss = 0.06477573\n",
            "Iteration 195, loss = 0.06465973\n",
            "Iteration 196, loss = 0.06454527\n",
            "Iteration 197, loss = 0.06443233\n",
            "Iteration 198, loss = 0.06432085\n",
            "Iteration 199, loss = 0.06421081\n",
            "Iteration 200, loss = 0.06410217\n",
            "Iteration 201, loss = 0.06399489\n",
            "Iteration 202, loss = 0.06388895\n",
            "Iteration 203, loss = 0.06378432\n",
            "Iteration 204, loss = 0.06368100\n",
            "Iteration 205, loss = 0.06357896\n",
            "Iteration 206, loss = 0.06347804\n",
            "Iteration 207, loss = 0.06337824\n",
            "Iteration 208, loss = 0.06327965\n",
            "Iteration 209, loss = 0.06318232\n",
            "Iteration 210, loss = 0.06308609\n",
            "Iteration 211, loss = 0.06299091\n",
            "Iteration 212, loss = 0.06289673\n",
            "Iteration 213, loss = 0.06280360\n",
            "Iteration 214, loss = 0.06271155\n",
            "Iteration 215, loss = 0.06262049\n",
            "Iteration 216, loss = 0.06253036\n",
            "Iteration 217, loss = 0.06244113\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20,), learning_rate_init=0.01, max_iter=300,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(20,), learning_rate_init=0.01, max_iter=300,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20,), learning_rate_init=0.01, max_iter=300,\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Treinamento \n",
        "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
        "print('Treinando...')\n",
        "#model = MLPClassifier()  #com hiperametros default, para ver quais são consulte https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "#model = MLPClassifier(verbose=True)  #Exibindo o treinamento, onde iteração é a época e loss é o erro quadrado médio\n",
        "#'''\n",
        "model = MLPClassifier(\n",
        "    hidden_layer_sizes=(20,),\n",
        "    max_iter=300,\n",
        "    verbose=True,\n",
        "    learning_rate='constant',\n",
        "    learning_rate_init=0.01,\n",
        "    activation='relu',\n",
        "    solver = 'adam'\n",
        ")\n",
        "#'''\n",
        "\n",
        "model.fit(entrada,saidaDesejada)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF78kzNdGyty",
        "outputId": "6c8f0abe-9adb-4ca0-c909-a7633316f0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Printando os dados das Plantas Iris para Teste\n",
            "['1.65', '20', '115', '42.2', 'Obesidade Morbida']\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m linha \u001b[39min\u001b[39;00m dados:\n\u001b[0;32m      9\u001b[0m   \u001b[39mprint\u001b[39m(linha)\n\u001b[1;32m---> 10\u001b[0m   coluna \u001b[39m=\u001b[39m linha\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m   \u001b[39mprint\u001b[39m (coluna[\u001b[39m0\u001b[39m])\n\u001b[0;32m     12\u001b[0m   \u001b[39mprint\u001b[39m (coluna[\u001b[39m0\u001b[39m])\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "#Carga dos dados de Teste\n",
        "entrada =[]\n",
        "saidaDesejada = []\n",
        "\n",
        "print(\"Printando os dados das Plantas Iris para Teste\")\n",
        "with open('DadosDeTeste.csv') as csvfile:\n",
        "  dados = csv.reader(csvfile)\n",
        "  for linha in dados:\n",
        "    print(linha)\n",
        "    coluna = linha.split(',')\n",
        "    print (coluna[0])\n",
        "    print (coluna[0])\n",
        "    print (coluna[1])\n",
        "    print (coluna[2])\n",
        "    print (coluna[3])\n",
        "    entrada.append([float(coluna[0]),float(coluna[3])])\n",
        "    saidaDesejada.append(coluna[4].replace('\"',''))\n",
        "\n",
        "print(\"Quantidade de dados: \", len(entrada))\n",
        "for ind in range(0,len(entrada)):\n",
        "  print(\"Features de entrada: \", entrada[ind],\"-\", \"Classe: \" + saidaDesejada[ind])\n",
        "\n",
        "csvfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UCzESm-QETo",
        "outputId": "077082d6-c5cc-41a6-fc59-9b7b32072227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testando...\n",
            "Predição:  ['Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa'\n",
            " 'Setosa' 'Setosa' 'Versicolor' 'Versicolor' 'Versicolor' 'Versicolor'\n",
            " 'Versicolor' 'Versicolor' 'Versicolor' 'Versicolor' 'Versicolor'\n",
            " 'Versicolor' 'Virginica' 'Virginica' 'Virginica' 'Virginica' 'Virginica'\n",
            " 'Virginica' 'Virginica' 'Virginica' 'Virginica' 'Virginica']\n",
            "[[9.99683502e-01 3.16498187e-04 6.80873861e-19]\n",
            " [9.98731307e-01 1.26869311e-03 4.31924845e-17]\n",
            " [9.99474754e-01 5.25246297e-04 9.21970736e-18]\n",
            " [9.98691031e-01 1.30896919e-03 1.68177199e-16]\n",
            " [9.99761225e-01 2.38775304e-04 4.99557056e-19]\n",
            " [9.99335300e-01 6.64699760e-04 3.40396207e-18]\n",
            " [9.99355576e-01 6.44423961e-04 3.28733155e-17]\n",
            " [9.99426305e-01 5.73694924e-04 4.92449853e-18]\n",
            " [9.98347406e-01 1.65259449e-03 5.47622373e-16]\n",
            " [9.99154946e-01 8.45053721e-04 1.44852877e-17]\n",
            " [4.28061911e-04 9.99396199e-01 1.75739238e-04]\n",
            " [5.55921679e-04 9.98218513e-01 1.22556499e-03]\n",
            " [2.73631279e-04 9.95449156e-01 4.27721318e-03]\n",
            " [1.28930701e-03 9.84388225e-01 1.43224678e-02]\n",
            " [3.82619724e-04 9.88543112e-01 1.10742681e-02]\n",
            " [8.72983716e-04 9.81518378e-01 1.76086386e-02]\n",
            " [3.92376160e-04 9.88261776e-01 1.13458479e-02]\n",
            " [9.59124959e-03 9.90361242e-01 4.75087771e-05]\n",
            " [5.86416011e-04 9.98744375e-01 6.69208568e-04]\n",
            " [1.64834398e-03 9.89284409e-01 9.06724729e-03]\n",
            " [3.59131964e-09 2.33052631e-04 9.99766944e-01]\n",
            " [3.11557375e-07 2.54743670e-03 9.97452252e-01]\n",
            " [2.68087772e-08 1.11215039e-03 9.98887823e-01]\n",
            " [3.13970406e-07 4.08518565e-03 9.95914500e-01]\n",
            " [1.31316003e-08 4.64189213e-04 9.99535798e-01]\n",
            " [2.46833211e-09 2.61485586e-04 9.99738512e-01]\n",
            " [5.90986240e-06 1.25392527e-02 9.87454837e-01]\n",
            " [4.00908201e-08 1.71934116e-03 9.98280619e-01]\n",
            " [4.38447896e-08 8.97879098e-04 9.99102077e-01]\n",
            " [2.96843116e-09 2.87160393e-04 9.99712837e-01]]\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Generalização - Teste com Y\n",
        "print('Testando...')\n",
        "\n",
        "saidaPredita = model.predict(entrada)\n",
        "print('Predição: ', saidaPredita)\n",
        "\n",
        "print(model.predict_proba(entrada))\n",
        "#print(model.score(entrada,saidaDesejada,sample_weight=None))\n",
        "\n",
        "mat = confusion_matrix(saidaDesejada, saidaPredita)\n",
        "print(mat)\n",
        "accuracy_score(saidaDesejada, saidaPredita)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
